{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_PATH = \"IT_Grundschutz_Kompendium_Edition2022.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "from itertools import pairwise\n",
    "from typing import List, Optional, Union\n",
    "import json\n",
    "import re\n",
    "\n",
    "import pdfminer\n",
    "import pdfminer.high_level\n",
    "\n",
    "from spacy.lang.char_classes import ALPHA, HYPHENS, PUNCT, CONCAT_QUOTES\n",
    "from spacy.lang.de import German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_ON_PAGE_IDX = 92\n",
    "pdf_pages = list(pdfminer.high_level.extract_pages(PDF_PATH, laparams=pdfminer.layout.LAParams(boxes_flow=None)))[START_ON_PAGE_IDX:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "box2str = lambda box: box.get_text().strip()\n",
    "is_text_box = lambda element: isinstance(element, pdfminer.layout.LTTextBoxHorizontal)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Page:\n",
    "    page_num: int\n",
    "    baustein_abbrev: Optional[str]\n",
    "    body_fragments: List[str]\n",
    "\n",
    "    @property\n",
    "    def is_empty(self) -> bool:\n",
    "        return not self.baustein_abbrev or not self.body_fragments\n",
    "\n",
    "    @classmethod\n",
    "    def from_pdfminer_page(cls, page: pdfminer.layout.LTPage) -> 'Page':\n",
    "        elements = list(filter(is_text_box, page))\n",
    "        is_empty = len(elements) < 2\n",
    "        if is_empty:\n",
    "            return cls(page_num=page.pageid, baustein_abbrev=None, body_fragments=[])\n",
    "        _, abbrev = elements[:2]\n",
    "        body_elements = elements[2:-2]  # exclude footer content\n",
    "        return cls(page_num=page.pageid, baustein_abbrev=box2str(abbrev), body_fragments=list(map(box2str, body_elements)))\n",
    "\n",
    "pages = [Page.from_pdfminer_page(pdf_page) for pdf_page in pdf_pages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BausteinPageRange:\n",
    "    abbrev: str\n",
    "    pages: List[Page]\n",
    "\n",
    "\n",
    "def baustein_page_ranges_from_pages(pages: List[Page]) -> List[BausteinPageRange]:\n",
    "    baustein_page_ranges = []\n",
    "\n",
    "    current_baustein_page_range = BausteinPageRange(abbrev='DUMMY', pages=[])\n",
    "    for page in pages:\n",
    "        if page.is_empty:\n",
    "            continue\n",
    "\n",
    "        found_new_abbrev = current_baustein_page_range.abbrev != page.baustein_abbrev\n",
    "        if found_new_abbrev:\n",
    "            current_baustein_page_range = BausteinPageRange(abbrev=page.baustein_abbrev, pages=[page])\n",
    "            baustein_page_ranges.append(current_baustein_page_range)\n",
    "        else:\n",
    "            current_baustein_page_range.pages.append(page)\n",
    "    \n",
    "    return baustein_page_ranges\n",
    "\n",
    "baustein_page_ranges = baustein_page_ranges_from_pages(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_CHAR = f\"({HYPHENS}|{PUNCT}|[{ALPHA}{CONCAT_QUOTES}./\\d])\"\n",
    "TEXT_CHAR = f\"({WORD_CHAR}|\\s)\"\n",
    "NEWLINE = r\"\\n\"\n",
    "\n",
    "RE_BAUSTEIN_ABBREV = re.compile(r\"[A-Z]{3,4}\\.\\d+(\\.\\d+)*\")\n",
    "RE_BAUSTEIN_HEADING = re.compile(RE_BAUSTEIN_ABBREV.pattern + \":? \")\n",
    "RE_REQUIREMENT_HEADING = re.compile(RE_BAUSTEIN_ABBREV.pattern + \"\\.A\\d+\")\n",
    "RE_RESPONSIBLE_FUNCTION_LIST = re.compile(f\"\\[{TEXT_CHAR}+\\]\")\n",
    "RE_SECURITY_LEVEL = re.compile(r\"\\((B|S|H)\\)\")\n",
    "RE_REQUIREMENT_HEADING_FULL = re.compile(RE_REQUIREMENT_HEADING.pattern + f\"{TEXT_CHAR}+{RE_SECURITY_LEVEL.pattern}\")\n",
    "RE_SECTION_HEADING = re.compile(\"\\d(\\.\\d+)*\\.?\")\n",
    "RE_LIST_ITEM = re.compile(\"•\")\n",
    "RE_CROSS_REFERENCE_SECTION_HEADING = re.compile(\"5\\.? Anlage\")\n",
    "\n",
    "RE_SUB_HYPHENS = re.compile(f\"(?P<ante>{WORD_CHAR})({HYPHENS}){NEWLINE}(?P<post>{WORD_CHAR})\")\n",
    "\n",
    "RE_DO_NOT_MERGE_ACROSS_PAGES = re.compile(\"(\" + \"|\".join([\n",
    "    RE_BAUSTEIN_HEADING.pattern,\n",
    "    RE_REQUIREMENT_HEADING.pattern,\n",
    "    RE_SECTION_HEADING.pattern,\n",
    "    \"G \\d+\\.\",  # threat heading\n",
    "    RE_LIST_ITEM.pattern,\n",
    "    \"X\",  # entries in cross reference tables\n",
    "    # concrete headings:\n",
    "    \"CIA-Werte\",\n",
    "    \"Zuständigkeiten\",\n",
    "]) + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Baustein:\n",
    "    abbrev: str\n",
    "    name: str\n",
    "    requirements: List['Requirement']\n",
    "\n",
    "    @classmethod\n",
    "    def from_baustein_page_range(cls, baustein_page_range: BausteinPageRange) -> 'Baustein':\n",
    "        return BausteinParser(baustein_page_range).parse()\n",
    "\n",
    "    @property\n",
    "    def title(self) -> str:\n",
    "        return f\"{self.abbrev}: {self.name}\"\n",
    "\n",
    "    def to_md(self) -> str:\n",
    "        md = f\"# {self.title}\\n\\n\"\n",
    "\n",
    "        for requirement in self.requirements:\n",
    "            md += requirement.to_md()\n",
    "        \n",
    "        md += \"\\n\"\n",
    "        return md\n",
    "\n",
    "    def to_simple_dict(self) -> dict:\n",
    "        return dict(\n",
    "            abbrev=self.abbrev,\n",
    "            name=self.name,\n",
    "            requirements=[req.to_simple_dict() for req in self.requirements]\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Requirement:\n",
    "    name: str\n",
    "    statements: List[Union['StatementString', 'StatementListItem']]\n",
    "\n",
    "    def to_md(self) -> str:\n",
    "        md = f\"## {self.name}\\n\\n\"\n",
    "        \n",
    "        for statement in self.statements:\n",
    "            md += statement.to_md()\n",
    "        \n",
    "        md += \"\\n\"\n",
    "        return md\n",
    "\n",
    "    def to_simple_dict(self) -> dict:\n",
    "        return dict(\n",
    "            name=self.name,\n",
    "            statements=[stmt.to_simple_dict() for stmt in self.statements]\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StatementBase:\n",
    "    body: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StatementString(StatementBase):\n",
    "    def to_md(self) -> str:\n",
    "        return f\"- {self.body}\\n\"\n",
    "\n",
    "    def to_simple_dict(self) -> dict:\n",
    "        return dict(\n",
    "            body=self.body,\n",
    "            type=\"string\"\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StatementListItem(StatementBase):\n",
    "    def to_md(self) -> str:\n",
    "        return f\"    - {self.body}\\n\"\n",
    "\n",
    "    def to_simple_dict(self) -> dict:\n",
    "        return dict(\n",
    "            body=self.body,\n",
    "            type=\"list_item\"\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tag(Enum):\n",
    "    baustein_heading = 'baustein_heading'\n",
    "    section_heading = 'section_heading'\n",
    "    requirement_heading = 'requirement_heading'\n",
    "    body = 'body'\n",
    "    list_item = 'list_item'\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Para:\n",
    "    s: str\n",
    "    tag: Tag\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.tag:<25}{self.s}\"\n",
    "\n",
    "\n",
    "class CrossReferenceTableReached(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "nlp = German()\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "\n",
    "class BausteinParser:\n",
    "    def __init__(self, baustein_page_range):\n",
    "        self._baustein_page_range = baustein_page_range\n",
    "        self._raw_paras =[]\n",
    "        self._paras = []\n",
    "\n",
    "    @staticmethod\n",
    "    def should_merge_across_page_boundary(last_fragment_of_prev_page: str, first_fragment_of_next_page: str) -> bool:\n",
    "        last, first = last_fragment_of_prev_page, first_fragment_of_next_page\n",
    "\n",
    "        if RE_DO_NOT_MERGE_ACROSS_PAGES.match(first):\n",
    "            return False\n",
    "\n",
    "        return not BausteinParser.is_sentence_boundary_between_fragments(last, first)\n",
    "\n",
    "    @staticmethod\n",
    "    def is_sentence_boundary_between_fragments(fragment_1: str, fragment_2: str) -> bool:\n",
    "        # join the fragments and ask spacy if there's a sentence boundary where we joined.\n",
    "        combined_fragments = f\"{fragment_1}\\n{fragment_2}\"\n",
    "        start_of_first = len(fragment_1)\n",
    "        doc = nlp(combined_fragments)\n",
    "        span = doc.char_span(start_of_first, start_of_first + 2, alignment_mode=\"expand\")\n",
    "        return span[0].is_sent_start\n",
    "\n",
    "    @staticmethod\n",
    "    def split_fragment_into_sentences(fragment):\n",
    "        return [sent.text for sent in nlp(fragment).sents]\n",
    "\n",
    "    def parse(self) -> Baustein:\n",
    "        self.extract_raw_paragraphs()\n",
    "        self.sanitize()\n",
    "        return self.parse_baustein()\n",
    "\n",
    "    def extract_raw_paragraphs(self) -> None:\n",
    "        did_merge = False\n",
    "        for page, next_page in pairwise(self._baustein_page_range.pages):\n",
    "            page_fragments = page.body_fragments\n",
    "            next_page_first_fragment = next_page.body_fragments[0]\n",
    "\n",
    "            try:\n",
    "                if self.should_merge_across_page_boundary(page_fragments[-1], next_page_first_fragment):\n",
    "                    self._extract_paragraphs_from_page(page, skip_first=did_merge, append=next_page_first_fragment)\n",
    "                    did_merge = True\n",
    "                else:\n",
    "                    self._extract_paragraphs_from_page(page, skip_first=did_merge)\n",
    "                    did_merge = False\n",
    "            except CrossReferenceTableReached:\n",
    "                return\n",
    "    \n",
    "    def sanitize(self) -> None:\n",
    "        self._tag_raw_paras()\n",
    "        self._fixup_requirement_headings()\n",
    "        self._remove_bullets_and_fixup_list_items()\n",
    "        self._dehyphenate()\n",
    "        self._replace_newlines()\n",
    "\n",
    "    def parse_baustein(self) -> Baustein:\n",
    "        baustein = Baustein(abbrev=None, name=None, requirements=[])\n",
    "\n",
    "        requirement = None\n",
    "        for para in self._paras:\n",
    "            if para.tag == Tag.baustein_heading:\n",
    "                abbrev_match = RE_BAUSTEIN_ABBREV.match(para.s)\n",
    "                baustein.abbrev = abbrev_match.group()\n",
    "                baustein.name = para.s[abbrev_match.end() + 1:].strip()\n",
    "            if requirement and para.tag in (Tag.section_heading, Tag.requirement_heading):\n",
    "                requirement = None\n",
    "            if para.tag == Tag.requirement_heading:\n",
    "                requirement = Requirement(name=para.s, statements=[])\n",
    "                baustein.requirements.append(requirement)\n",
    "            if requirement and para.tag == Tag.body:\n",
    "                for sentence in self.split_fragment_into_sentences(para.s):\n",
    "                    requirement.statements.append(StatementString(sentence))\n",
    "            if requirement and para.tag == Tag.list_item:\n",
    "                requirement.statements.append(StatementListItem(para.s))\n",
    "\n",
    "        return baustein\n",
    "\n",
    "    def _dehyphenate(self) -> None:\n",
    "        for idx, para in enumerate(self._paras):\n",
    "            while re.match(f\"({HYPHENS})\", para.s[-1]):\n",
    "                next = self._paras.pop(idx + 1)\n",
    "                para.s += \"\\n\" + next.s\n",
    "            para.s = RE_SUB_HYPHENS.sub(r\"\\g<ante>\\g<post>\", para.s)\n",
    "\n",
    "    def _extract_paragraphs_from_page(self, page: Page, skip_first: bool, append: str = \"\") -> None:\n",
    "        start_idx = 1 if skip_first else 0\n",
    "        paras = page.body_fragments[start_idx:-1] + [page.body_fragments[-1] + append]\n",
    "        for idx, para in enumerate(paras):\n",
    "            # stop when reaching the cross-reference tables (which are hard to parse)\n",
    "            if RE_CROSS_REFERENCE_SECTION_HEADING.match(para):\n",
    "                self._raw_paras.extend(paras[:idx])\n",
    "                raise CrossReferenceTableReached()\n",
    "        self._raw_paras.extend(paras)\n",
    "\n",
    "    def _fixup_requirement_headings(self) -> None:\n",
    "        for idx, para in enumerate(self._paras):\n",
    "            if para.tag != Tag.requirement_heading:\n",
    "                continue\n",
    "            while (\n",
    "                not RE_REQUIREMENT_HEADING_FULL.match(para.s)\n",
    "                or (\"[\" in para.s and \"]\" not in para.s)\n",
    "            ):\n",
    "                next = self._paras.pop(idx + 1)\n",
    "                para.s += \"\\n\" + next.s\n",
    "\n",
    "    def _remove_bullets_and_fixup_list_items(self) -> None:\n",
    "        for idx, para in enumerate(self._paras):\n",
    "            if not para.tag == Tag.list_item:\n",
    "                continue\n",
    "\n",
    "            para.s = re.sub(\"^•\\s+\", \"\", para.s)\n",
    "            if idx < len(self._paras) - 1:\n",
    "                next = self._paras[idx + 1]\n",
    "                if next.tag == Tag.body and not self.is_sentence_boundary_between_fragments(para.s, next.s):\n",
    "                    self._paras.pop(idx + 1)\n",
    "                    para.s += \"\\n\" + next.s\n",
    "\n",
    "    def _replace_newlines(self) -> None:\n",
    "        for para in self._paras:\n",
    "            para.s = para.s.replace(\"\\n\", \" \")\n",
    "\n",
    "    def _tag_raw_paras(self) -> None:\n",
    "        for raw_para in self._raw_paras:\n",
    "            if RE_BAUSTEIN_HEADING.match(raw_para):\n",
    "                tag = Tag.baustein_heading\n",
    "            elif RE_SECTION_HEADING.match(raw_para):\n",
    "                tag = Tag.section_heading\n",
    "            elif RE_REQUIREMENT_HEADING.match(raw_para):\n",
    "                tag = Tag.requirement_heading\n",
    "            elif RE_LIST_ITEM.match(raw_para):\n",
    "                tag = Tag.list_item\n",
    "            else:\n",
    "                tag = Tag.body\n",
    "            self._paras.append(Para(s=raw_para, tag=tag))\n",
    "\n",
    "bausteine = [BausteinParser(baustein_page_range).parse() for baustein_page_range in baustein_page_ranges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "baustein_dicts = []\n",
    "baustein_md_links = []\n",
    "\n",
    "dir = \"bausteine_2022\"\n",
    "\n",
    "for baustein in bausteine:\n",
    "    fname = f\"{baustein.abbrev}.md\"\n",
    "    baustein_md_links.append(f\"- [{baustein.title}]({fname})\")\n",
    "\n",
    "    with open(f\"{dir}/{fname}\", \"w\") as file:\n",
    "        file.write(baustein.to_md())\n",
    "    baustein_dicts.append(baustein.to_simple_dict())\n",
    "\n",
    "with open(f\"{dir}/_index.md\", \"w\") as md_index:\n",
    "    md_index.write(\"\\n\".join(baustein_md_links) + \"\\n\")\n",
    "\n",
    "with open(f\"{dir}/bausteine.json\", \"w\") as json_file:\n",
    "    json.dump(baustein_dicts, json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e78cb18402433016681753f081e90b08629789f85a598d8d7c00b73386dde0f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('it-grundschutz-bausteine-aooQhTVM')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
